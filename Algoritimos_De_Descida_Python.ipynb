{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff455ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import approx_fprime as gradient\n",
    "from busca_ln_goldstein import busca_linear_goldstein\n",
    "from busca_ln_wolfe import busca_linear_wolfe\n",
    "from busca_ln_armijo import busca_linear_armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea91eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metodo do gradiente com armijo\n",
    "def gradiente_ln_armijo(f, x):\n",
    "    g = gradient(x, f, epsilon=1e-5)\n",
    "    n = 1e-2\n",
    "    y = 0.5\n",
    "    num_backtrack = 0\n",
    "    iter = 0\n",
    "    while np.linalg.norm(g) > 1e-10:\n",
    "        d = -g\n",
    "        t = 1\n",
    "        result, t, k = busca_linear_armijo(f, x, d, t, y, n)\n",
    "        if not result:\n",
    "            print(\"Não Converge\")\n",
    "            return -1\n",
    "        xt= x + t * d\n",
    "        x=xt\n",
    "        g = gradient(x, f, epsilon=1e-5)\n",
    "        iter += 1\n",
    "        num_backtrack += k\n",
    "        if iter > 100000:\n",
    "            print(\"Não Converge\")\n",
    "            return -1\n",
    "    \n",
    "    return x, iter, num_backtrack\n",
    "\n",
    "\n",
    "# metodo do gradiente com wolfe\n",
    "def gradiente_ln_wolfe(f, x):\n",
    "    g = gradient(x, f, epsilon=1e-5)\n",
    "    n1 = 1e-2\n",
    "    n2 = 1e-1\n",
    "    y = 0.5 # fator de redução de t\n",
    "    num_backtrack = 0\n",
    "    iter = 0\n",
    "    while np.linalg.norm(g) > 1e-10:\n",
    "        d = -g\n",
    "        t = 1\n",
    "        result, t, k = busca_linear_wolfe(f, x, d, t, y, n1, n2)\n",
    "        if not result:\n",
    "            print(\"Não Converge\")\n",
    "            return -1\n",
    "        xt= x + t * d\n",
    "        x=xt\n",
    "        g = gradient(x, f, epsilon=1e-5)\n",
    "        iter += 1\n",
    "        num_backtrack += k\n",
    "        if iter > 100000:\n",
    "            print(\"Não Converge\")\n",
    "            return -1\n",
    "    \n",
    "    return x, iter, num_backtrack\n",
    "\n",
    "def gradiente_ln_Goldstein(f, x):\n",
    "    g = gradient(x, f, epsilon=1e-5)\n",
    "    n = 1e-4\n",
    "    y = 0.5 # fator de redução de t\n",
    "    num_backtrack = 0\n",
    "    iter = 0\n",
    "    while np.linalg.norm(g) > 1e-10:\n",
    "        d = -g\n",
    "        t = 1\n",
    "        result, t, k = busca_linear_goldstein(f, x, d, t, y, n)\n",
    "        if not result:\n",
    "            print(\"Não Converge\")\n",
    "            return -1\n",
    "        xt= x + t * d\n",
    "        x=xt\n",
    "        g = gradient(x, f, epsilon=1e-5)\n",
    "        iter += 1\n",
    "        num_backtrack += k\n",
    "        if iter > 100000:\n",
    "            print(\"Não Converge\")\n",
    "            return -1\n",
    "    \n",
    "    return x, iter, num_backtrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d2c9e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------iteração = 0 -------------------------------------------\n",
      "\n",
      "f(    x    +    t    *    d    ) <= f(x)        +    n    *   t   * grad(f(     x     )) *    d    \n",
      "f(  [1 2]  +    1    *[ 0.999995 -2.00001 ]) <= f( [1 2]  ) +  0.01   *   1   * grad(f(   [1 2]   )) * [ 0.999995 -2.00001 ]\n",
      "f(  [1 2]  +[ 0.999995 -2.00001 ]) <=     1.5     +  0.01   *   1   * [-0.999995  2.00001 ] * [ 0.999995 -2.00001 ]\n",
      "f([ 1.99999500e+00 -1.00000008e-05]) <=     1.5     +  0.01   *   1   *       -5.000030000127483       \n",
      "       1.0000200001141548        <= 1.4499996999987252                                             \n",
      "\n",
      "----------------------------------------iteração = 0 -------------------------------------------\n",
      "\n",
      "f(    x    +    t    *    d    ) <= f(x)        +    n    *   t   * grad(f(     x     )) *    d    \n",
      "f([ 1.99999500e+00 -1.00000008e-05]+    1    *[-0.       2.00001]) <= f([ 1.99999500e+00 -1.00000008e-05]) +  0.01   *   1   * grad(f([ 1.99999500e+00 -1.00000008e-05])) * [-0.       2.00001]\n",
      "f([ 1.99999500e+00 -1.00000008e-05]+[-0.       2.00001]) <= 1.0000200001141548 +  0.01   *   1   * [ 0.      -2.00001]  * [-0.       2.00001]\n",
      "f(     [1.999995 2.      ]     ) <= 1.0000200001141548 +  0.01   *   1   *       -4.000040000155719       \n",
      "       1.0000000000387048        <= 0.9600196001125976                                             \n",
      "\n",
      "----------------------------------------iteração = 1 -------------------------------------------\n",
      "\n",
      "f(    x    +    t    *    d    ) <= f(x)        +    n    *   t   * grad(f(     x     )) *    d    \n",
      "f([ 1.99999500e+00 -1.00000008e-05]+   0.5   *[-0.       2.00001]) <= f([ 1.99999500e+00 -1.00000008e-05]) +  0.01   *  0.5  * grad(f([ 1.99999500e+00 -1.00000008e-05])) * [-0.       2.00001]\n",
      "f([ 1.99999500e+00 -1.00000008e-05]+[-0.        1.000005]) <= 1.0000200001141548 +  0.01   *  0.5  * [ 0.      -2.00001]  * [-0.       2.00001]\n",
      "f(     [1.999995 0.999995]     ) <= 1.0000200001141548 +  0.01   *  0.5  *       -4.000040000155719       \n",
      "     3.749994069352534e-11       <= 0.9800198001133762                                             \n",
      "(array([1.999995, 0.999995]), 2, 1)\n"
     ]
    }
   ],
   "source": [
    "# função de teste\n",
    "def f(x):\n",
    "    f = 0.5*(x[0] - 2)**2 + (x[1] - 1)**2\n",
    "    return f\n",
    "\n",
    "def g(x):\n",
    "    g = x[0]**2 + x[0]*x[1] + x[1]**2\n",
    "    return g\n",
    "\n",
    "# ponto inicial\n",
    "x = np.array([1, 2])\n",
    "\n",
    "\n",
    "print(gradiente_ln_armijo(f, x))\n",
    "#print(gradiente_ln_wolfe(f, x))\n",
    "#print(gradiente_ln_Goldstein(g, x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
